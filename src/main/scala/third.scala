import org.apache.spark.SparkContext


object third {
  def main(args: Array[String]): Unit = {
    import org.apache.spark.SparkContext



//    val sc=new SparkContext("local[*]","bigdata")
    //
    //
    //
    //        val rdd1=sc.textFile("/Users/kalky/Documents/file operators folder/ips.txt")
    //
    //        val rdd2=rdd1.flatMap(x=>x.split(","))
    //
    //        val rdd3=rdd2.map(x=>(x,1))
    //
    //        val rdd4=rdd3.reduceByKey((x,y)=>x+y)
    //
    //        val rdd5 = rdd4.sortBy(x=>x._2,false)
    //
    //        rdd5.take(1).foreach(println)
    ////      }
    import org.apache.spark.SparkContext
//
//    object ipadresscount {
//      def main(args: Array[String]):Unit ={
//
//    }
//    val sc=new SparkContext("local[*]","bigdata")
//
//
//    val rdd1=sc.textFile("/Users/kalky/Documents/file operators folder/newtxt.txt")
//
//
//    val rdd2=rdd1.flatMap(x=>x.split(" "))
//
//
//    val rdd3=rdd2.map(x=>(x,1))
//
//
//    val rdd4=rdd3.reduceByKey((x,y)=>x+y)
//
//
//    rdd4.collect().foreach(println)




  }
}
